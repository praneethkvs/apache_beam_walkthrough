{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install apache-beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "##class to split a csv line by elements and keep only the columns we are interested in \n",
    "class Split(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        Date,Open,High,Low,Close,Volume = element.split(',')\n",
    "        return [Open+\",\"+Close]\n",
    "\n",
    "##Define Pipeline\n",
    "def run_pipeline(input_file,output_file):\n",
    "    with beam.Pipeline() as p:\n",
    "        from_csv = (p | \n",
    "                    beam.io.ReadFromText(input_file,skip_header_lines=1) | \n",
    "                    beam.ParDo(Split()) |\n",
    "                    beam.io.WriteToText(output_file,file_name_suffix=\".csv\",header='Open,Close',shard_name_template=''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function annotate_downstream_side_inputs at 0x7f3850d8ca60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function fix_side_input_pcoll_coders at 0x7f3850d8cb70> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function lift_combiners at 0x7f3850d8cbf8> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function expand_sdf at 0x7f3850d8cc80> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function expand_gbk at 0x7f3850d8cd08> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function sink_flattens at 0x7f3850d8ce18> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function greedily_fuse at 0x7f3850d8cea0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function read_to_impulse at 0x7f3850d8cf28> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function impulse_to_input at 0x7f3850d8f048> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function inject_timer_pcollections at 0x7f3850d8f1e0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function sort_stages at 0x7f3850d8f268> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner_transforms:==================== <function window_pcollection_coders at 0x7f3850d8f2f0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
      "INFO:apache_beam.runners.portability.fn_api_runner:Created Worker handler <apache_beam.runners.portability.fn_api_runner.EmbeddedWorkerHandler object at 0x7f383719acc0> for environment urn: \"beam:env:embedded_python:v1\"\n",
      "\n",
      "INFO:apache_beam.runners.portability.fn_api_runner:Running (ref_AppliedPTransform_ReadFromText/Read/_SDFBoundedSourceWrapper/Impulse_5)+(((ReadFromText/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction)+(ReadFromText/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write))\n",
      "INFO:apache_beam.runners.portability.fn_api_runner:Running ((ref_PCollection_PCollection_1_split/Read)+((ReadFromText/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process)+(ref_AppliedPTransform_ParDo(Split)_7)))+(((ref_AppliedPTransform_WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1005>)_17)+(ref_AppliedPTransform_WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)_18))+(WriteToText/Write/WriteImpl/GroupByKey/Write))\n",
      "INFO:apache_beam.runners.portability.fn_api_runner:Running (ref_AppliedPTransform_WriteToText/Write/WriteImpl/DoOnce/Impulse_12)+((ref_AppliedPTransform_WriteToText/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2597>)_13)+(((ref_AppliedPTransform_WriteToText/Write/WriteImpl/DoOnce/Map(decode)_15)+((ref_AppliedPTransform_WriteToText/Write/WriteImpl/InitializeWrite_16)+(ref_PCollection_PCollection_7/Write)))+(ref_PCollection_PCollection_6/Write)))\n",
      "INFO:apache_beam.runners.portability.fn_api_runner:Running ((WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteToText/Write/WriteImpl/WriteBundles_23))+(ref_PCollection_PCollection_13/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner:Running ((ref_PCollection_PCollection_6/Read)+(ref_AppliedPTransform_WriteToText/Write/WriteImpl/PreFinalize_24))+(ref_PCollection_PCollection_14/Write)\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "INFO:apache_beam.runners.portability.fn_api_runner:Running (ref_PCollection_PCollection_6/Read)+(ref_AppliedPTransform_WriteToText/Write/WriteImpl/FinalizeWrite_25)\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Set input/output\n",
    "input_file = \"sp500_sample.csv\"\n",
    "output_file = \"output\"\n",
    "\n",
    "#Run Pipeline\n",
    "run_pipeline(input_file,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read created output file.\n",
    "import pandas as pd\n",
    "dat = pd.read_csv(\"output.csv\")\n",
    "\n",
    "dat.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
